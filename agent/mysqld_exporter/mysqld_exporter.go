package main

import (
	"context"
	//"crypto/tls"
	"database/sql"
	"fmt"

	//"io/ioutil"
	//"net/http"
	"os"
	"path"
	"strconv"

	//"strings"
	"time"
	//"bytes"
	//"reflect"
	"dbms/agent/mysqld_exporter/collector"
	"dbms/agent/mysqld_exporter/com"

	_ "github.com/go-sql-driver/mysql"
	//"github.com/percona/exporter_shared"
	"github.com/prometheus/client_golang/prometheus"
	//"github.com/prometheus/client_golang/prometheus/promhttp"
	"github.com/prometheus/common/log"
	"github.com/prometheus/common/version"

	//"github.com/prometheus/common/expfmt"
	"gopkg.in/alecthomas/kingpin.v2"
	//"gopkg.in/ini.v1"
	//"gopkg.in/yaml.v2"

	"github.com/pythonsite/yamlConfig"
)

// System variable params formatting.
// See: https://github.com/go-sql-driver/mysql#system-variables
const (
	sessionSettingsParam = `log_slow_filter=%27tmp_table_on_disk,filesort_on_disk%27`
	timeoutParam         = `lock_wait_timeout=%d`
)

var (
	showVersion = kingpin.Flag(
		"version",
		"Print version information.",
	).Default("false").Bool()
	listenAddress = kingpin.Flag(
		"web.listen-address",
		"Address to listen on for web interface and telemetry.",
	).Default(":9104").String()
	metricPath = kingpin.Flag(
		"web.telemetry-path",
		"Path under which to expose metrics.",
	).Default("/metrics").String()
	timeoutOffset = kingpin.Flag(
		"timeout-offset",
		"Offset to subtract from timeout in seconds.",
	).Default("0.25").Float64()
	configMycnf = kingpin.Flag(
		"config.my-cnf",
		"Path to .my.cnf file to read MySQL credentials from.",
	).Default(path.Join(os.Getenv("HOME"), ".my.cnf")).String()
	exporterLockTimeout = kingpin.Flag(
		"exporter.lock_wait_timeout",
		"Set a lock_wait_timeout on the connection to avoid long metadata locking.",
	).Default("2").Int()
	exporterLogSlowFilter = kingpin.Flag(
		"exporter.log_slow_filter",
		"Add a log_slow_filter to avoid slow query logging of scrapes. NOTE: Not supported by Oracle MySQL.",
	).Default("false").Bool()
	exporterGlobalConnPool = kingpin.Flag(
		"exporter.global-conn-pool",
		"Use global connection pool instead of creating new pool for each http request.",
	).Default("true").Bool()
	exporterMaxOpenConns = kingpin.Flag(
		"exporter.max-open-conns",
		"Maximum number of open connections to the database. https://golang.org/pkg/database/sql/#DB.SetMaxOpenConns",
	).Default("3").Int()
	exporterMaxIdleConns = kingpin.Flag(
		"exporter.max-idle-conns",
		"Maximum number of connections in the idle connection pool. https://golang.org/pkg/database/sql/#DB.SetMaxIdleConns",
	).Default("3").Int()
	exporterConnMaxLifetime = kingpin.Flag(
		"exporter.conn-max-lifetime",
		"Maximum amount of time a connection may be reused. https://golang.org/pkg/database/sql/#DB.SetConnMaxLifetime",
	).Default("1m").Duration()
	collectAll = kingpin.Flag(
		"collect.all",
		"Collect all metrics.",
	).Default("false").Bool()

	dsn string
)

// scrapers lists all possible collection methods and if they should be enabled by default.
var scrapers = map[collector.Scraper]bool{
	collector.ScrapeGlobalStatus{}:                        true,
	collector.ScrapeGlobalVariables{}:                     false,
	collector.ScrapeSlaveStatus{}:                         false,
	collector.ScrapeProcesslist{}:                         false,
	collector.ScrapeTableSchema{}:                         false,
	collector.ScrapeInfoSchemaInnodbTablespaces{}:         false,
	collector.ScrapeInnodbMetrics{}:                       false,
	collector.ScrapeAutoIncrementColumns{}:                false,
	collector.ScrapeBinlogSize{}:                          false,
	collector.ScrapePerfTableIOWaits{}:                    false,
	collector.ScrapePerfIndexIOWaits{}:                    false,
	collector.ScrapePerfTableLockWaits{}:                  false,
	collector.ScrapePerfEventsStatements{}:                false,
	collector.ScrapePerfEventsWaits{}:                     false,
	collector.ScrapePerfFileEvents{}:                      false,
	collector.ScrapePerfFileInstances{}:                   false,
	collector.ScrapeUserStat{}:                            false,
	collector.ScrapeClientStat{}:                          false,
	collector.ScrapeTableStat{}:                           false,
	collector.ScrapeQueryResponseTime{}:                   false,
	collector.ScrapeEngineTokudbStatus{}:                  false,
	collector.ScrapeEngineInnodbStatus{}:                  false,
	collector.ScrapeHeartbeat{}:                           false,
	collector.ScrapeInnodbCmp{}:                           false,
	collector.ScrapeInnodbCmpMem{}:                        false,
	collector.ScrapeCustomQuery{Resolution: collector.HR}: false,
	collector.ScrapeCustomQuery{Resolution: collector.MR}: false,
	collector.ScrapeCustomQuery{Resolution: collector.LR}: false,
	collector.NewStandardGo():                             false,
	collector.NewStandardProcess():                        false,
}

// TODO Remove
var scrapersHr = map[collector.Scraper]struct{}{
	collector.ScrapeGlobalStatus{}:                        {},
	collector.ScrapeInnodbMetrics{}:                       {},
	collector.ScrapeCustomQuery{Resolution: collector.HR}: {},
}

// TODO Remove
var scrapersMr = map[collector.Scraper]struct{}{
	collector.ScrapeSlaveStatus{}:                         {},
	collector.ScrapeProcesslist{}:                         {},
	collector.ScrapePerfEventsWaits{}:                     {},
	collector.ScrapePerfFileEvents{}:                      {},
	collector.ScrapePerfTableLockWaits{}:                  {},
	collector.ScrapeQueryResponseTime{}:                   {},
	collector.ScrapeEngineInnodbStatus{}:                  {},
	collector.ScrapeInnodbCmp{}:                           {},
	collector.ScrapeInnodbCmpMem{}:                        {},
	collector.ScrapeCustomQuery{Resolution: collector.MR}: {},
}

// TODO Remove
var scrapersLr = map[collector.Scraper]struct{}{
	collector.ScrapeGlobalVariables{}:                     {},
	collector.ScrapeTableSchema{}:                         {},
	collector.ScrapeAutoIncrementColumns{}:                {},
	collector.ScrapeBinlogSize{}:                          {},
	collector.ScrapePerfTableIOWaits{}:                    {},
	collector.ScrapePerfIndexIOWaits{}:                    {},
	collector.ScrapePerfFileInstances{}:                   {},
	collector.ScrapeUserStat{}:                            {},
	collector.ScrapeTableStat{}:                           {},
	collector.ScrapePerfEventsStatements{}:                {},
	collector.ScrapeClientStat{}:                          {},
	collector.ScrapeInfoSchemaInnodbTablespaces{}:         {},
	collector.ScrapeEngineTokudbStatus{}:                  {},
	collector.ScrapeHeartbeat{}:                           {},
	collector.ScrapeCustomQuery{Resolution: collector.LR}: {},
}

type ServerConfig struct {
	Host     string
	Port     int
	Username string
	Password string
	Dbname   string
	Timeout  int
}

func GetDsn() string {

	currdir, _ := os.Getwd()
	yamlFile := currdir + "/etc/config.yml"

	config := yamlConfig.ConfigEngine{}
	err := config.Load(yamlFile)
	if err != nil {
		log.Fatalln("Config load error:", err)
	}

	serverconf := ServerConfig{}
	res := config.GetStruct("mysql", &serverconf)
	//fmt.Printf("%v",res)
	log.Debugln(res)
	host := serverconf.Host
	port := strconv.Itoa(serverconf.Port)
	username := serverconf.Username
	password := serverconf.Password
	dbname := serverconf.Dbname

	dsn := username + ":" + password + "@(" + host + ":" + port + ")/" + dbname + "?timeout=5s"
	//fmt.Println("dsn: " + dsn)
	log.Debugln(dsn)
	return dsn
}

func enabledScrapers(scraperFlags map[collector.Scraper]*bool) (all, hr, mr, lr []collector.Scraper) {
	for scraper, enabled := range scraperFlags {
		if *collectAll || *enabled {
			if _, ok := scrapers[scraper]; ok {
				all = append(all, scraper)
			}
			if _, ok := scrapersHr[scraper]; ok {
				hr = append(hr, scraper)
			}
			if _, ok := scrapersMr[scraper]; ok {
				mr = append(mr, scraper)
			}
			if _, ok := scrapersLr[scraper]; ok {
				lr = append(lr, scraper)
			}
		}
	}

	return all, hr, mr, lr
}

func newDB(dsn string) (*sql.DB, error) {
	// Validate DSN, and open connection pool.
	db, err := sql.Open("mysql", dsn)
	if err != nil {
		return nil, err
	}

	db.SetMaxOpenConns(*exporterMaxOpenConns)
	db.SetMaxIdleConns(*exporterMaxIdleConns)
	db.SetConnMaxLifetime(*exporterConnMaxLifetime)

	return db, nil
}

func init() {
	prometheus.MustRegister(version.NewCollector("mysqld_exporter"))
}

func newHandler(maindb *sql.DB, dbid int, dbinst *sql.DB, metrics collector.Metrics, scrapers []collector.Scraper, defaultGatherer bool) {
	filteredScrapers := scrapers

	//ctx := r.Context()
	ctx, _ := context.WithCancel(context.Background())

	if dbinst == nil || maindb == nil {
		log.Fatalln("Error opening connection to database: ", strconv.Itoa(dbid))
		return
	}

	//var err error
	rows, err := maindb.Query("select key_, label, item_id, value_type from pms_items where obj_id = ?", dbid)
	if err != nil {
		log.Infoln("Get item template error: ", err)
	}

	registry := prometheus.NewRegistry()
	registry.MustRegister(collector.New(ctx, dbinst, metrics, filteredScrapers))

	//count,_ :=prometheus.GetMetricWith("")

	gatherers := prometheus.Gatherers{}
	if defaultGatherer {
		gatherers = append(gatherers, prometheus.DefaultGatherer)
	}
	gatherers = append(gatherers, registry)

	gathering, err := gatherers.Gather()
	if err != nil {
		fmt.Println(err)
		return
	}

	m0 := make(map[string]int)
	m1 := make(map[string]int)
	var (
		key        string
		label      string
		item_id    int
		value_type int
	)
	var key_label string
	for rows.Next() {
		err = rows.Scan(&key, &label, &item_id, &value_type)
		if err != nil {
			log.Infoln(err)
		}
		key_label = key + "_" + label
		m0[key_label] = item_id
		m1[key_label] = value_type
	}
	defer rows.Close()

	gather_time := time.Now().Unix()
	fmt.Printf("gather_time: %d\n", gather_time)
	//out := &bytes.Buffer{}
	for _, mf := range gathering {
		//fmt.Println(reflect.TypeOf(mf))   type: io_prometheus_client
		//fmt.Print(mf.GetName() + "\n")
		//fmt.Print(mf.GetHelp() + "\n")
		//fmt.Print(mf.GetMetric())
		//fmt.Print("\n")

		mfname := mf.GetName()

		//if mfname == "mysql_up" {
		fmt.Print(mfname + "\n")

		mr := mf.GetMetric()
		for _, m := range mr {
			// 获取LabelPair类型值
			ml := m.GetLabel()
			var lvalue string
			for _, mli := range ml {
				//lname := mli.GetName()
				lvalue = mli.GetValue()

				//fmt.Print(lname + "\n")
				fmt.Print(lvalue + "\n")
			}

			k_label := mfname + "_" + lvalue
			fmt.Print("key_label: " + k_label + "\n")
			itemid := m0[k_label]
			val_type := m1[k_label]
			fmt.Print(itemid)
			fmt.Print("\n")
			fmt.Print(val_type)
			fmt.Print("\n")

			if itemid > 0 {
				//根据值的类型取值
				var mr_value float64
				if val_type == 1 {
					// 获取Counter类型值
					mr_value = m.GetCounter().GetValue()
					fmt.Printf("%f\n", mr_value)

				} else if val_type == 2 {
					// 获取Gauge类型值
					mr_value = m.GetGauge().GetValue()
					fmt.Printf("%f\n", mr_value)

				} else if val_type == 3 {
					// 获取Untyped类型值
					mr_value = m.GetUntyped().GetValue()
					fmt.Printf("%f\n", mr_value)
				} else if val_type == 4 {
					// 获取Histogram类型值
					//mh := m.GetSummary()
					//fmt.Print(mh)
					//fmt.Print("\n")
				} else if val_type == 5 {
					// 获取Summary类型值
					//ms := m.GetHistogram()
					//fmt.Print(ms)
					//fmt.Print("\n")
				}

				//保存结果到maindb中
				_, err = maindb.Exec("INSERT INTO pms_item_data(itemid,time,value)VALUES (?,?,?)", itemid, gather_time, mr_value)
				if err != nil {
					fmt.Println("insert failed,", err)
				}
			}

		}

		//}

		// if _, err := expfmt.MetricFamilyToText(out, mf); err != nil {
		// 	panic(err)
		// }
	}
	//fmt.Print(out.String())
	fmt.Println("----------")
}

func main() {
	// Generate ON/OFF flags for all scrapers.
	scraperFlags := map[collector.Scraper]*bool{}
	for scraper, enabledByDefault := range scrapers {
		f := kingpin.Flag(
			"collect."+scraper.Name(),
			scraper.Help(),
		).Default(strconv.FormatBool(enabledByDefault)).Bool()

		scraperFlags[scraper] = f
	}

	// Parse flags.
	kingpin.Parse()

	if *showVersion {
		fmt.Fprintln(os.Stdout, version.Print("mysqld_exporter"))
		os.Exit(0)
	}

	log.Infoln("Starting mysqld_exporter", version.Info())
	log.Infoln("Build context", version.BuildContext())

	// Get Server Center dsn
	dsn := GetDsn()
	log.Infoln("dsn: ", dsn)

	// Open global connection pool if requested.
	var maindb *sql.DB
	var err error
	if *exporterGlobalConnPool {
		maindb, err = newDB(dsn)
		if err != nil {
			log.Fatalln("Error opening connection to main database:", err)
			return
		}

		err = maindb.Ping()
		if err != nil {
			log.Fatalln("Error opening connection to main database:", err)
			return
		}

		defer maindb.Close()
	}

	all, _, _, _ := enabledScrapers(scraperFlags)

	var (
		dbid     int
		host     string
		port     int
		username string
		password string
		dbname   string
	)
	var mysql_dsn string
	var dbinst *sql.DB
	for {
		log.Infoln("check mysql controller start.")
		rows, err := maindb.Query("select id, host, port, username, password, db_name from pms_db_config where db_type = 2 and status = 1 and is_delete = 0")
		if err != nil {
			log.Infoln("No mysql exists.")
		} else {
			for rows.Next() {
				err = rows.Scan(&dbid, &host, &port, &username, &password, &dbname)
				if err != nil {
					log.Infoln(err)
				}
				mysql_dsn = username + ":" + password + "@(" + host + ":" + strconv.Itoa(port) + ")/" + dbname + "?timeout=5s"
				log.Infoln(mysql_dsn)
				dbinst, err = newDB(mysql_dsn)
				if err != nil {
					log.Errorln("Error opening connection to mysql instance: ", err)
					com.ErrorConnection(dbid, maindb)
					continue
				} else {
					err = dbinst.Ping()
					if err != nil {
						log.Errorln("Error opening connection to mysql instance: ", err)
						com.ErrorConnection(dbid, maindb)
						continue
					}
					newHandler(maindb, dbid, dbinst, collector.NewMetrics(""), all, true)
				}
				defer dbinst.Close()

			}
		}
		defer rows.Close()
		log.Infoln("check mysql controller finished.")

		time.Sleep(1 * time.Minute)
	}

}
